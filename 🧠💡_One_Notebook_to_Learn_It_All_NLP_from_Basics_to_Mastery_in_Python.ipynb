{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 📚 Basic NLP Terminology Explained\n",
        "\n",
        "- **Corpus**:  \n",
        "  A corpus is a collection of text — usually **paragraphs or full documents**.  \n",
        "  *Example:* A folder of news articles or a dataset of tweets.\n",
        "\n",
        "- **Documents**:  \n",
        "  Documents are individual pieces within a corpus. They often represent **a single article or paragraph**.  \n",
        "  *Example:* One product review or one blog post.\n",
        "\n",
        "- **Sentences**:  \n",
        "  Each document is made up of **sentences**.  \n",
        "  *Example:* \"I love this product. It works great.\"\n",
        "\n",
        "- **Words**:  \n",
        "  Sentences are made up of **simple words**.  \n",
        "  *Example:* \"I\", \"love\", \"this\", \"product\".\n",
        "\n",
        "- **Vocabulary**:  \n",
        "  The vocabulary is the set of **unique words** found in the entire corpus.  \n",
        "  *Example:* If your corpus has the words \"apple\", \"banana\", \"apple\", the vocabulary is `{\"apple\", \"banana\"}`.\n"
      ],
      "metadata": {
        "id": "E7F6Vh12oBDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🔹 Key Terminologies in NLP\n",
        "\n",
        "#### **Tokenization**\n",
        "Splitting text into smaller units (tokens) like words, subwords, or characters.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Stopwords**\n",
        "Common words (like \"the,\" \"and,\" \"is\") that are often ignored in NLP tasks because they don’t contribute much to meaning.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Stemming**\n",
        "Reducing words to their root form.  \n",
        "**Example:** `\"running\"` → `\"run\"`\n",
        "\n",
        "---\n",
        "\n",
        "#### **Lemmatization**\n",
        "Reducing words to their base or dictionary form.  \n",
        "**Example:** `\"better\"` → `\"good\"`\n",
        "\n",
        "---\n",
        "\n",
        "#### **Part-of-Speech (POS) Tagging**\n",
        "Identifying the grammatical parts of speech in a sentence (e.g., noun, verb, adjective).  \n",
        "**Example:** `\"She runs quickly\"` → `[\"She\" (Pronoun), \"runs\" (Verb), \"quickly\" (Adverb)]`\n",
        "\n",
        "---\n",
        "\n",
        "#### **Named Entity Recognition (NER)**\n",
        "Identifying and classifying proper nouns (e.g., names of people, organizations, locations).  \n",
        "**Example:** `\"Hashmi Bhatt is from Gujrat\"` → `\"Hashmi Bhatt\"` (Person), `\"Gujrat\"` (Location)\n",
        "\n",
        "---\n",
        "\n",
        "#### **Dependency Parsing**\n",
        "Analyzing the grammatical structure of a sentence to establish relationships between words.  \n",
        "**Example:** `\"I love programming\"` → `\"I\"` (Subject), `\"love\"` (Verb), `\"programming\"` (Object)\n",
        "\n",
        "---\n",
        "\n",
        "#### **Bag of Words (BoW)**\n",
        "A simple model used to represent text as an unordered collection of words, disregarding grammar and word order but keeping track of word frequency.\n",
        "\n",
        "---\n",
        "\n",
        "#### **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
        "A statistical measure used to evaluate the importance of a word in a document relative to all other documents in a corpus.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Word Embeddings**\n",
        "A technique to represent words in continuous vector space, where similar words have similar vector representations.  \n",
        "**Common methods:** Word2Vec, GloVe, FastText\n"
      ],
      "metadata": {
        "id": "gcWbOuGEoiP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧩 Tokenization in NLP\n",
        "\n",
        "### 🔹 What is Tokenization?\n",
        "Tokenization is the process of breaking text into smaller units like **sentences**, **words**, or **characters**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 Types of Tokenization\n",
        "\n",
        "- **Sentence Tokenization**  \n",
        "  Splits text into individual sentences.  \n",
        "  📚 *Library:* `nltk.sent_tokenize`, `spaCy`\n",
        "\n",
        "- **Word Tokenization**  \n",
        "  Splits sentences into words.  \n",
        "  📚 *Library:* `nltk.word_tokenize`, `spaCy`, `TextBlob`\n",
        "\n",
        "- **Subword Tokenization**  \n",
        "  Breaks words into smaller parts (useful in deep learning).  \n",
        "  📚 *Library:* `SentencePiece`, `Byte-Pair Encoding (BPE)`, Hugging Face's `tokenizers`\n",
        "\n",
        "- **Character Tokenization**  \n",
        "  Splits text into individual characters.  \n",
        "  📚 *Library:* Simple Python code or custom logic\n",
        "\n",
        "---\n",
        "\n",
        "### ✅ Summary Table\n",
        "\n",
        "| Type            | Description                     | Common Libraries                  |\n",
        "|-----------------|---------------------------------|-----------------------------------|\n",
        "| Sentence        | Text → Sentences                | `nltk`, `spaCy`                   |\n",
        "| Word            | Sentences → Words               | `nltk`, `spaCy`, `TextBlob`       |\n",
        "| Subword         | Words → Subword units           | `SentencePiece`, `tokenizers`     |\n",
        "| Character       | Text → Characters               | Python string methods             |\n"
      ],
      "metadata": {
        "id": "NjD-7sbTo9JR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJwLpml2g8hv",
        "outputId": "d5152af5-0d88-4683-885e-5df18a8e354e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tjs5Z5U0lCrr",
        "outputId": "d05bd3c9-308a-4ac5-e03a-9ea351d1c998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus=\"\"\"Mary closed on closing night when she was in the mood to close.\n",
        "Mary had a little lamb.\n",
        "Her fleece was white as snow\"\"\"\n"
      ],
      "metadata": {
        "id": "EE7PEv_Nj2oS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-r2b-zzktND",
        "outputId": "2e33feca-48f1-4184-f07a-809d235dc3d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mary closed on closing night when she was in the mood to close. Mary had a little lamb. Her fleece was white as snow\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##  Tokenization\n",
        "## Sentence-->paragraphs\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "m5LhjpX_kvZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d1 = sent_tokenize(corpus)\n",
        "print(type(d1))\n",
        "d1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5h9cpEckzkY",
        "outputId": "7e892944-4b15-44a4-9474-d3099e897ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mary closed on closing night when she was in the mood to close.',\n",
              " 'Mary had a little lamb.',\n",
              " 'Her fleece was white as snow']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d2 = word_tokenize(corpus)\n",
        "print(type(d2))\n",
        "d2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQmWBbN0lRXG",
        "outputId": "63d44ff8-9d5b-4eb2-d9d3-635c978e800a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mary',\n",
              " 'closed',\n",
              " 'on',\n",
              " 'closing',\n",
              " 'night',\n",
              " 'when',\n",
              " 'she',\n",
              " 'was',\n",
              " 'in',\n",
              " 'the',\n",
              " 'mood',\n",
              " 'to',\n",
              " 'close',\n",
              " '.',\n",
              " 'Mary',\n",
              " 'had',\n",
              " 'a',\n",
              " 'little',\n",
              " 'lamb',\n",
              " '.',\n",
              " 'Her',\n",
              " 'fleece',\n",
              " 'was',\n",
              " 'white',\n",
              " 'as',\n",
              " 'snow']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPUjFL1Ik2pS",
        "outputId": "91456cc5-b58b-445d-a389-ccfd21f2f5ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mary',\n",
              " 'closed',\n",
              " 'on',\n",
              " 'closing',\n",
              " 'night',\n",
              " 'when',\n",
              " 'she',\n",
              " 'was',\n",
              " 'in',\n",
              " 'the',\n",
              " 'mood',\n",
              " 'to',\n",
              " 'close',\n",
              " '.',\n",
              " 'Mary',\n",
              " 'had',\n",
              " 'a',\n",
              " 'little',\n",
              " 'lamb',\n",
              " '.',\n",
              " 'Her',\n",
              " 'fleece',\n",
              " 'was',\n",
              " 'white',\n",
              " 'as',\n",
              " 'snow']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=TreebankWordTokenizer()\n",
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtPILsh8lWs0",
        "outputId": "997763b5-757b-4413-b306-4db0a2089a38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Mary',\n",
              " 'closed',\n",
              " 'on',\n",
              " 'closing',\n",
              " 'night',\n",
              " 'when',\n",
              " 'she',\n",
              " 'was',\n",
              " 'in',\n",
              " 'the',\n",
              " 'mood',\n",
              " 'to',\n",
              " 'close.',\n",
              " 'Mary',\n",
              " 'had',\n",
              " 'a',\n",
              " 'little',\n",
              " 'lamb.',\n",
              " 'Her',\n",
              " 'fleece',\n",
              " 'was',\n",
              " 'white',\n",
              " 'as',\n",
              " 'snow']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🌱 Stemming Process in NLP\n",
        "\n",
        "### 🔹 What is Stemming?\n",
        "Stemming is the process of **reducing a word to its root form** by removing prefixes or suffixes.\n",
        "\n",
        "- Example:  \n",
        "  `\"playing\"`, `\"played\"`, `\"plays\"` → `\"play\"`\n",
        "\n",
        "> ⚠️ Note: The stemmed word may not always be a valid word in English.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 Why Use Stemming?\n",
        "- Helps in **text normalization**\n",
        "- Reduces **dimensionality** of the data\n",
        "- Groups related words together during analysis\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 Common Stemming Algorithms & Libraries\n",
        "\n",
        "| Stemmer               | Description                                 | Library         |\n",
        "|-----------------------|---------------------------------------------|------------------|\n",
        "| **Porter Stemmer**     | Most common, rule-based, fast and simple     | `nltk.PorterStemmer` |\n",
        "| **Lancaster Stemmer**  | More aggressive than Porter                  | `nltk.LancasterStemmer` |\n",
        "| **Snowball Stemmer**   | An improvement over Porter (supports multiple languages) | `nltk.SnowballStemmer` |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2XRCRleTsGwS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
      ],
      "metadata": {
        "id": "wU2Fec4np5dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "j2zzYcTkmweX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemming=PorterStemmer()"
      ],
      "metadata": {
        "id": "nYh0VoT5puwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word+\"---->\"+stemming.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5w_pn1opxUW",
        "outputId": "54c05c7a-aba5-4f5c-c551-1a2dc58bbb6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eaten\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->histori\n",
            "finally---->final\n",
            "finalized---->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer"
      ],
      "metadata": {
        "id": "CX0-gOPoqB2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$', min=4)"
      ],
      "metadata": {
        "id": "yLEYeop0qXjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem('eating')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "--C1qAK7qY9F",
        "outputId": "5ec982cd-ef78-4f52-d9e0-4849eb8446e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem('ingeating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-84hdi_Cqd-k",
        "outputId": "df4e0e33-3a0c-4560-a821-2ad205b81f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ingeat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "HWlsErf-qf5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowballsstemmer=SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "ml8TX9EwqzsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word+\"---->\"+snowballsstemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLFUExRaq88d",
        "outputId": "ce636ec9-7ca5-4ca2-b902-34b8bbbc5e0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eaten\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->histori\n",
            "finally---->final\n",
            "finalized---->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# diffrence btw PorterStemmer and SnowBallStemmer\n",
        "# Portar\n",
        "stemming.stem(\"fairly\"),stemming.stem(\"sportingly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf-_r5wzq_DP",
        "outputId": "e80f12c8-beb5-48bb-ae82-485d4b96f650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairli', 'sportingli')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# diffrence btw PorterStemmer and SnowBallStemmer\n",
        "# SnowBall\n",
        "snowballsstemmer.stem(\"fairly\"),snowballsstemmer.stem(\"sportingly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dI--1wFPrEz0",
        "outputId": "61b36fca-20f9-48f3-8452-eda04a424986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fair', 'sport')"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🌿 Lemmatization Process in NLP\n",
        "\n",
        "### 🔹 What is Lemmatization?\n",
        "Lemmatization reduces a word to its **base or dictionary form** (called a *lemma*), considering the **context and part of speech**.\n",
        "\n",
        "- Example:  \n",
        "  `\"better\"` → `\"good\"`  \n",
        "  `\"running\"` → `\"run\"` (as a verb)\n",
        "\n",
        "> ✅ Lemmatization returns **real words** and is more accurate than stemming.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 Why Use Lemmatization?\n",
        "- More accurate normalization than stemming  \n",
        "- Helps in better **semantic analysis**  \n",
        "- Preserves meaning by considering grammar rules\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 Common Lemmatization Tools & Libraries\n",
        "\n",
        "| Tool               | Description                                 | Library         |\n",
        "|--------------------|---------------------------------------------|------------------|\n",
        "| **WordNet Lemmatizer** | Uses WordNet dictionary to find base form  | `nltk.WordNetLemmatizer` |\n",
        "| **spaCy Lemmatizer**   | Advanced lemmatizer with POS tagging built-in | `spaCy` |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "TMcSnka-tOjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9yJ5J9wsyaP",
        "outputId": "30d4ecce-ec5e-4a89-d70c-62d3f32c1f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "j4TtWj2YrGTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "k8Nr6H6Vspd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "POS- Noun-n\n",
        "verb-v\n",
        "adjective-a\n",
        "adverb-r\n",
        "'''\n",
        "lemmatizer.lemmatize(\"going\",pos='v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "9D4JTR7isrCZ",
        "outputId": "61c54e82-3e53-483d-a062-2599539255f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"going\",pos='a')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "16Rom2mTtEMS",
        "outputId": "79dd02fe-c549-44e3-ccd8-b87cf30b5d8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'going'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"going\",pos='n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "hHBz5lbftEeq",
        "outputId": "c232f38b-64b0-4c9b-fbcc-cf29427b68d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'going'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word+\"---->\"+lemmatizer.lemmatize(word,pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_P-NgLWsvnO",
        "outputId": "53f2b2d5-ba84-411f-f4e6-a70f83f6c220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eat\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->history\n",
            "finally---->finally\n",
            "finalized---->finalize\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🚫 Stopwords in NLP\n",
        "\n",
        "### 🔹 What are Stopwords?\n",
        "Stopwords are **common words** (like \"the\", \"and\", \"is\") that are often **ignored** in text processing because they don’t add significant meaning.\n",
        "\n",
        "- Example:  \n",
        "  \"The quick brown fox jumps over the lazy dog.\"  \n",
        "  Stopwords: `\"The\", \"over\", \"the\"`\n",
        "\n",
        "> ✅ **Stopwords removal** helps reduce noise in data and improve model efficiency.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 Why Remove Stopwords?\n",
        "- **Reduce dimensionality** by ignoring words with little semantic value\n",
        "- Helps **focus on meaningful words** for tasks like sentiment analysis, topic modeling, etc.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "83ByavxZwOEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Speech Of Nikola Tesla\n",
        "paragraph = \"\"\"I have scarcely had courage enough to address an audience on a few unavoidable occasions, and the experience of this evening, even as disconnected from the cause of our meeting, is quite novel to me.\n",
        "Although in those few instances, of which I have retained agreeable memory, my words have met with a generous reception, I never deceived myself, and knew quite well that my success was not due to any excellency in the rhetorical or demonstrative art.\n",
        "Nevertheless, my sense of duty to respond to the request with which I was honored a few days ago was strong enough to overcome my very grave apprehensions in regard to my ability of doing justice to the topic assigned to me.\n",
        "It is true, at times—even now, as I speak—my mind feels full of the subject, but I know that, as soon as I shall attempt expression, the fugitive conceptions will vanish, and I shall experience certain well known sensations of abandonment, chill and silence.\n",
        "I can see already your disappointed countenances and can read in them the painful regret of the mistake in your choice.\"\"\""
      ],
      "metadata": {
        "id": "3w2PssuMs5QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "jc_WA9s6t1qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZHAyYzSumnl",
        "outputId": "fb2f5472-cc4d-44fc-d51b-c8155f3fa11c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkaLiztxup8x",
        "outputId": "2eef0f73-2903-4040-89e6-e013d00c91ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "0mrRyezUvdd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76sYjCy1vfC6",
        "outputId": "aa154046-21af-49ec-e223-625eb3637813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have scarcely had courage enough to address an audience on a few unavoidable occasions, and the experience of this evening, even as disconnected from the cause of our meeting, is quite novel to me.',\n",
              " 'Although in those few instances, of which I have retained agreeable memory, my words have met with a generous reception, I never deceived myself, and knew quite well that my success was not due to any excellency in the rhetorical or demonstrative art.',\n",
              " 'Nevertheless, my sense of duty to respond to the request with which I was honored a few days ago was strong enough to overcome my very grave apprehensions in regard to my ability of doing justice to the topic assigned to me.',\n",
              " 'It is true, at times—even now, as I speak—my mind feels full of the subject, but I know that, as soon as I shall attempt expression, the fugitive conceptions will vanish, and I shall experience certain well known sensations of abandonment, chill and silence.',\n",
              " 'I can see already your disappointed countenances and can read in them the painful regret of the mistake in your choice.']"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowballstemmer=SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "RSmdDuCovshp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply Stopwords And Filter And then Apply Snowball Stemming\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    words=nltk.word_tokenize(sentences[i])\n",
        "    words=[snowballstemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i]=' '.join(words)# converting all the list of words into sentences"
      ],
      "metadata": {
        "id": "Ce-YXWscuudB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eD7LSmxlvtgm",
        "outputId": "200e9c53-ec93-474f-ed35-0c66d4ade809"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i scarc courag enough address audienc unavoid occas , experi even , even disconnect caus meet , quit novel .',\n",
              " 'although instanc , i retain agreeabl memori , word met generous recept , i never deceiv , knew quit well success due excel rhetor demonstr art .',\n",
              " 'nevertheless , sens duti respond request i honor day ago strong enough overcom grave apprehens regard abil justic topic assign .',\n",
              " 'it true , times—even , i speak—mi mind feel full subject , i know , soon i shall attempt express , fugit concept vanish , i shall experi certain well known sensat abandon , chill silenc .',\n",
              " 'i see alreadi disappoint counten read pain regret mistak choic .']"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "HXjAapnOvwLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Apply Stopwords And Filter And then Apply Snowball Stemming\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    #sentences[i]=sentences[i].lower()\n",
        "    words=nltk.word_tokenize(sentences[i])\n",
        "    words=[lemmatizer.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i]=' '.join(words)# converting all the list of words into sentences"
      ],
      "metadata": {
        "id": "nZHYuNetv3Zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfRMZCGKv47a",
        "outputId": "cadc37b8-4de2-44d6-b0ad-bd53bffbf9c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scarc courag enough address audienc unavoid occas , experi even , even disconnect caus meet , quit novel .',\n",
              " 'although instanc , retain agreeabl memori , word meet generous recept , never deceiv , know quit well success due excel rhetor demonstr art .',\n",
              " 'nevertheless , sens duti respond request honor day ago strong enough overcom grave apprehens regard abil justic topic assign .',\n",
              " 'true , times—even , speak—mi mind feel full subject , know , soon shall attempt express , fugit concept vanish , shall experi certain well know sensat abandon , chill silenc .',\n",
              " 'see alreadi disappoint counten read pain regret mistak choic .']"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🏷️ Part-of-Speech (POS) Tagging in NLP\n",
        "\n",
        "### 🔹 What is POS Tagging?\n",
        "Part-of-Speech (POS) tagging is the process of **labeling each word** in a sentence with its **grammatical role**, like noun, verb, adjective, etc.\n",
        "\n",
        "- Example:  \n",
        "  `\"She runs quickly\"` →  \n",
        "  `\"She\" (Pronoun), \"runs\" (Verb), \"quickly\" (Adverb)`\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 Why Use POS Tagging?\n",
        "- Helps in **understanding sentence structure**\n",
        "- Useful for **syntax analysis**, **lemmatization**, **NER**, and **translation**\n",
        "- Provides **contextual meaning** to words\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 Common POS Tags\n",
        "\n",
        "| Tag  | Meaning       | Example        |\n",
        "|------|---------------|----------------|\n",
        "| NN   | Noun          | dog, book      |\n",
        "| VB   | Verb          | run, eat       |\n",
        "| JJ   | Adjective     | quick, blue    |\n",
        "| RB   | Adverb        | quickly, very  |\n",
        "| PRP  | Pronoun       | he, she        |\n",
        "| IN   | Preposition   | on, at         |\n",
        "| DT   | Determiner    | the, a         |\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 Libraries for POS Tagging\n",
        "\n",
        "| Library  | Function                         |\n",
        "|----------|----------------------------------|\n",
        "| **NLTK** | `nltk.pos_tag()`                 |\n",
        "| **spaCy**| Built-in POS tagging pipeline    |\n",
        "| **TextBlob** | `textblob.tags`             |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Sg-gxawiw_Cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "''' ALL POS TAGs\n",
        "CC coordinating conjunction\n",
        "CD cardinal digit\n",
        "DT determiner\n",
        "EX existential there (like: “there is” … think of it like “there exists”)\n",
        "FW foreign word\n",
        "IN preposition/subordinating conjunction\n",
        "JJ adjective – ‘big’\n",
        "JJR adjective, comparative – ‘bigger’\n",
        "JJS adjective, superlative – ‘biggest’\n",
        "LS list marker 1)\n",
        "MD modal – could, will\n",
        "NN noun, singular ‘- desk’\n",
        "NNS noun plural – ‘desks’\n",
        "NNP proper noun, singular – ‘Harrison’\n",
        "NNPS proper noun, plural – ‘Americans’\n",
        "PDT predeterminer – ‘all the kids’\n",
        "POS possessive ending parent’s\n",
        "PRP personal pronoun –  I, he, she\n",
        "PRP$ possessive pronoun – my, his, hers\n",
        "RB adverb – very, silently,\n",
        "RBR adverb, comparative – better\n",
        "RBS adverb, superlative – best\n",
        "RP particle – give up\n",
        "TO – to go ‘to’ the store.\n",
        "UH interjection – errrrrrrrm\n",
        "VB verb, base form – take\n",
        "VBD verb, past tense – took\n",
        "VBG verb, gerund/present participle – taking\n",
        "VBN verb, past participle – taken\n",
        "VBP verb, sing. present, non-3d – take\n",
        "VBZ verb, 3rd person sing. present – takes\n",
        "WDT wh-determiner – which\n",
        "WP wh-pronoun – who, what\n",
        "WP$ possessive wh-pronoun, eg- whose\n",
        "WRB wh-adverb, eg- where, when\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "q0uIjQ4Ov6wH",
        "outputId": "cf08029c-fe09-44f1-c457-ff2b139565ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nCC coordinating conjunction \\nCD cardinal digit \\nDT determiner \\nEX existential there (like: “there is” … think of it like “there exists”) \\nFW foreign word \\nIN preposition/subordinating conjunction \\nJJ adjective – ‘big’ \\nJJR adjective, comparative – ‘bigger’ \\nJJS adjective, superlative – ‘biggest’ \\nLS list marker 1) \\nMD modal – could, will \\nNN noun, singular ‘- desk’ \\nNNS noun plural – ‘desks’ \\nNNP proper noun, singular – ‘Harrison’ \\nNNPS proper noun, plural – ‘Americans’ \\nPDT predeterminer – ‘all the kids’ \\nPOS possessive ending parent’s \\nPRP personal pronoun –  I, he, she \\nPRP$ possessive pronoun – my, his, hers \\nRB adverb – very, silently, \\nRBR adverb, comparative – better \\nRBS adverb, superlative – best \\nRP particle – give up \\nTO – to go ‘to’ the store. \\nUH interjection – errrrrrrrm \\nVB verb, base form – take \\nVBD verb, past tense – took \\nVBG verb, gerund/present participle – taking \\nVBN verb, past participle – taken \\nVBP verb, sing. present, non-3d – take \\nVBZ verb, 3rd person sing. present – takes \\nWDT wh-determiner – which \\nWP wh-pronoun – who, what \\nWP$ possessive wh-pronoun, eg- whose \\nWRB wh-adverb, eg- where, when\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Speech Of DR APJ Abdul Kalam\n",
        "paragraph = \"\"\"I have three visions for India. In 3000 years of our history, people from all over\n",
        "               the world have come and invaded us, captured our lands, conquered our minds.\n",
        "               From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\n",
        "               the French, the Dutch, all of them came and looted us, took over what was ours.\n",
        "               Yet we have not done this to any other nation. We have not conquered anyone.\n",
        "               We have not grabbed their land, their culture,\n",
        "               their history and tried to enforce our way of life on them.\n",
        "               Why? Because we respect the freedom of others.That is why my\n",
        "               first vision is that of freedom. I believe that India got its first vision of\n",
        "               this in 1857, when we started the War of Independence. It is this freedom that\n",
        "               we must protect and nurture and build on. If we are not free, no one will respect us.\n",
        "               My second vision for India’s development. For fifty years we have been a developing nation.\n",
        "               It is time we see ourselves as a developed nation. We are among the top 5 nations of the world\n",
        "               in terms of GDP. We have a 10 percent growth rate in most areas. Our poverty levels are falling.\n",
        "               Our achievements are being globally recognised today. Yet we lack the self-confidence to\n",
        "               see ourselves as a developed nation, self-reliant and self-assured. Isn’t this incorrect?\n",
        "               I have a third vision. India must stand up to the world. Because I believe that unless India\n",
        "               stands up to the world, no one will respect us. Only strength respects strength. We must be\n",
        "               strong not only as a military power but also as an economic power. Both must go hand-in-hand.\n",
        "               My good fortune was to have worked with three great minds. Dr. Vikram Sarabhai of the Dept. of\n",
        "               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.\n",
        "               I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.\n",
        "               I see four milestones in my career\"\"\""
      ],
      "metadata": {
        "id": "u4izK7gHxa5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "sentences=nltk.sent_tokenize(paragraph)"
      ],
      "metadata": {
        "id": "Pzcb8Iwfxk2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7jnC4ltx_Vn",
        "outputId": "bd68e8e6-6536-4ed0-c5d1-3ae34028db9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I have three visions for India.',\n",
              " 'In 3000 years of our history, people from all over \\n               the world have come and invaded us, captured our lands, conquered our minds.',\n",
              " 'From Alexander onwards, the Greeks, the Turks, the Moguls, the Portuguese, the British,\\n               the French, the Dutch, all of them came and looted us, took over what was ours.',\n",
              " 'Yet we have not done this to any other nation.',\n",
              " 'We have not conquered anyone.',\n",
              " 'We have not grabbed their land, their culture, \\n               their history and tried to enforce our way of life on them.',\n",
              " 'Why?',\n",
              " 'Because we respect the freedom of others.That is why my \\n               first vision is that of freedom.',\n",
              " 'I believe that India got its first vision of \\n               this in 1857, when we started the War of Independence.',\n",
              " 'It is this freedom that\\n               we must protect and nurture and build on.',\n",
              " 'If we are not free, no one will respect us.',\n",
              " 'My second vision for India’s development.',\n",
              " 'For fifty years we have been a developing nation.',\n",
              " 'It is time we see ourselves as a developed nation.',\n",
              " 'We are among the top 5 nations of the world\\n               in terms of GDP.',\n",
              " 'We have a 10 percent growth rate in most areas.',\n",
              " 'Our poverty levels are falling.',\n",
              " 'Our achievements are being globally recognised today.',\n",
              " 'Yet we lack the self-confidence to\\n               see ourselves as a developed nation, self-reliant and self-assured.',\n",
              " 'Isn’t this incorrect?',\n",
              " 'I have a third vision.',\n",
              " 'India must stand up to the world.',\n",
              " 'Because I believe that unless India \\n               stands up to the world, no one will respect us.',\n",
              " 'Only strength respects strength.',\n",
              " 'We must be \\n               strong not only as a military power but also as an economic power.',\n",
              " 'Both must go hand-in-hand.',\n",
              " 'My good fortune was to have worked with three great minds.',\n",
              " 'Dr. Vikram Sarabhai of the Dept.',\n",
              " 'of \\n               space, Professor Satish Dhawan, who succeeded him and Dr. Brahm Prakash, father of nuclear material.',\n",
              " 'I was lucky to have worked with all three of them closely and consider this the great opportunity of my life.',\n",
              " 'I see four milestones in my career']"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9SWK2TdyBA0",
        "outputId": "1a65cfcd-68de-45fb-be82-7054b9f42a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## We will find the Pos Tag\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    words=nltk.word_tokenize(sentences[i])\n",
        "    words=[word for word in words if word not in set(stopwords.words('english'))]\n",
        "    #sentences[i]=' '.join(words)# converting all the list of words into sentences\n",
        "    pos_tag=nltk.pos_tag(words)\n",
        "    print(pos_tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VO9XwstyDpr",
        "outputId": "dad00b76-c8e8-49e6-aa97-9b7f41c82695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('I', 'PRP'), ('three', 'CD'), ('visions', 'NNS'), ('India', 'NNP'), ('.', '.')]\n",
            "[('In', 'IN'), ('3000', 'CD'), ('years', 'NNS'), ('history', 'NN'), (',', ','), ('people', 'NNS'), ('world', 'NN'), ('come', 'VBP'), ('invaded', 'VBN'), ('us', 'PRP'), (',', ','), ('captured', 'VBD'), ('lands', 'NNS'), (',', ','), ('conquered', 'VBD'), ('minds', 'NNS'), ('.', '.')]\n",
            "[('From', 'IN'), ('Alexander', 'NNP'), ('onwards', 'NNS'), (',', ','), ('Greeks', 'NNP'), (',', ','), ('Turks', 'NNP'), (',', ','), ('Moguls', 'NNP'), (',', ','), ('Portuguese', 'NNP'), (',', ','), ('British', 'NNP'), (',', ','), ('French', 'NNP'), (',', ','), ('Dutch', 'NNP'), (',', ','), ('came', 'VBD'), ('looted', 'JJ'), ('us', 'PRP'), (',', ','), ('took', 'VBD'), ('.', '.')]\n",
            "[('Yet', 'RB'), ('done', 'VBN'), ('nation', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('conquered', 'VBD'), ('anyone', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('grabbed', 'VBD'), ('land', 'NN'), (',', ','), ('culture', 'NN'), (',', ','), ('history', 'NN'), ('tried', 'VBD'), ('enforce', 'JJ'), ('way', 'NN'), ('life', 'NN'), ('.', '.')]\n",
            "[('Why', 'WRB'), ('?', '.')]\n",
            "[('Because', 'IN'), ('respect', 'NN'), ('freedom', 'NN'), ('others.That', 'IN'), ('first', 'JJ'), ('vision', 'NN'), ('freedom', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('believe', 'VBP'), ('India', 'NNP'), ('got', 'VBD'), ('first', 'JJ'), ('vision', 'NN'), ('1857', 'CD'), (',', ','), ('started', 'VBD'), ('War', 'NNP'), ('Independence', 'NNP'), ('.', '.')]\n",
            "[('It', 'PRP'), ('freedom', 'NN'), ('must', 'MD'), ('protect', 'VB'), ('nurture', 'NN'), ('build', 'NN'), ('.', '.')]\n",
            "[('If', 'IN'), ('free', 'JJ'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
            "[('My', 'PRP$'), ('second', 'JJ'), ('vision', 'NN'), ('India', 'NNP'), ('’', 'NNP'), ('development', 'NN'), ('.', '.')]\n",
            "[('For', 'IN'), ('fifty', 'JJ'), ('years', 'NNS'), ('developing', 'VBG'), ('nation', 'NN'), ('.', '.')]\n",
            "[('It', 'PRP'), ('time', 'NN'), ('see', 'VB'), ('developed', 'JJ'), ('nation', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('among', 'IN'), ('top', 'JJ'), ('5', 'CD'), ('nations', 'NNS'), ('world', 'NN'), ('terms', 'NNS'), ('GDP', 'NNP'), ('.', '.')]\n",
            "[('We', 'PRP'), ('10', 'CD'), ('percent', 'JJ'), ('growth', 'NN'), ('rate', 'NN'), ('areas', 'NNS'), ('.', '.')]\n",
            "[('Our', 'PRP$'), ('poverty', 'NN'), ('levels', 'NNS'), ('falling', 'VBG'), ('.', '.')]\n",
            "[('Our', 'PRP$'), ('achievements', 'NNS'), ('globally', 'RB'), ('recognised', 'VBN'), ('today', 'NN'), ('.', '.')]\n",
            "[('Yet', 'RB'), ('lack', 'JJ'), ('self-confidence', 'NN'), ('see', 'NN'), ('developed', 'JJ'), ('nation', 'NN'), (',', ','), ('self-reliant', 'JJ'), ('self-assured', 'JJ'), ('.', '.')]\n",
            "[('Isn', 'NNP'), ('’', 'NNP'), ('incorrect', 'NN'), ('?', '.')]\n",
            "[('I', 'PRP'), ('third', 'JJ'), ('vision', 'NN'), ('.', '.')]\n",
            "[('India', 'NNP'), ('must', 'MD'), ('stand', 'VB'), ('world', 'NN'), ('.', '.')]\n",
            "[('Because', 'IN'), ('I', 'PRP'), ('believe', 'VBP'), ('unless', 'IN'), ('India', 'NNP'), ('stands', 'VBZ'), ('world', 'NN'), (',', ','), ('one', 'CD'), ('respect', 'NN'), ('us', 'PRP'), ('.', '.')]\n",
            "[('Only', 'RB'), ('strength', 'NN'), ('respects', 'NNS'), ('strength', 'NN'), ('.', '.')]\n",
            "[('We', 'PRP'), ('must', 'MD'), ('strong', 'JJ'), ('military', 'JJ'), ('power', 'NN'), ('also', 'RB'), ('economic', 'JJ'), ('power', 'NN'), ('.', '.')]\n",
            "[('Both', 'DT'), ('must', 'MD'), ('go', 'VB'), ('hand-in-hand', 'NN'), ('.', '.')]\n",
            "[('My', 'PRP$'), ('good', 'JJ'), ('fortune', 'NN'), ('worked', 'VBD'), ('three', 'CD'), ('great', 'JJ'), ('minds', 'NNS'), ('.', '.')]\n",
            "[('Dr.', 'NNP'), ('Vikram', 'NNP'), ('Sarabhai', 'NNP'), ('Dept', 'NNP'), ('.', '.')]\n",
            "[('space', 'NN'), (',', ','), ('Professor', 'NNP'), ('Satish', 'NNP'), ('Dhawan', 'NNP'), (',', ','), ('succeeded', 'VBD'), ('Dr.', 'NNP'), ('Brahm', 'NNP'), ('Prakash', 'NNP'), (',', ','), ('father', 'RB'), ('nuclear', 'JJ'), ('material', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('lucky', 'VBP'), ('worked', 'VBD'), ('three', 'CD'), ('closely', 'RB'), ('consider', 'VBP'), ('great', 'JJ'), ('opportunity', 'NN'), ('life', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('see', 'VBP'), ('four', 'CD'), ('milestones', 'NNS'), ('career', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " \"Taj Mahal is a beautiful Monument\".split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZsOPVSfyFXN",
        "outputId": "889b1d47-ad69-42a4-b6f7-20bf93fead4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Taj', 'Mahal', 'is', 'a', 'beautiful', 'Monument']"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(nltk.pos_tag(\"Taj Mahal is a beautiful Monument\".split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oh6l9vZvyL2v",
        "outputId": "b8fc0f61-0e73-464c-8732-0ca45b7ecdd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Taj', 'NNP'), ('Mahal', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('beautiful', 'JJ'), ('Monument', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🏷️ Named Entity Recognition (NER) in NLP\n",
        "\n",
        "### 🔹 What is Named Entity Recognition (NER)?\n",
        "Named Entity Recognition (NER) is the process of identifying and classifying **proper nouns** in a text into predefined categories, such as **persons**, **organizations**, **locations**, **dates**, etc.\n",
        "\n",
        "- Example:  \n",
        "  `\"Barack Obama is from Hawaii.\"`  \n",
        "  **Entities**:  \n",
        "  - \"Barack Obama\" (Person)  \n",
        "  - \"Hawaii\" (Location)\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 Why Use NER?\n",
        "- Extracts valuable information from text automatically\n",
        "- Helps in **structured data extraction** for tasks like knowledge graph creation and data analysis\n",
        "- Enables **question answering**, **summarization**, and **chatbots**\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 Common Named Entities Categories\n",
        "\n",
        "| Category       | Example                  |\n",
        "|----------------|--------------------------|\n",
        "| **Person**     | Barack Obama, Einstein   |\n",
        "| **Location**   | New York, Hawaii         |\n",
        "| **Organization** | Google, Microsoft       |\n",
        "| **Date**       | January 1, 2022          |\n",
        "| **Time**       | 5 PM, morning            |\n",
        "| **Money**      | $100, 50 euros           |\n",
        "| **Percent**    | 50%, 30 percent          |\n",
        "| **Miscellaneous** | Nobel Prize, Everest   |\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 Libraries for NER\n",
        "\n",
        "| Library      | Function                                 |\n",
        "|--------------|------------------------------------------|\n",
        "| **spaCy**    | Built-in NER model, highly efficient     |\n",
        "| **NLTK**     | Supports NER with pre-trained models    |\n",
        "| **Stanford NER** | Java-based NER library for fine-grained categories |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "IayVd-Ody2lO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy's pre-trained model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "id": "-VUc-hXBzrCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process a sentence\n",
        "sentence = \"The Eiffel Tower was built from 1887 to 1889 by Gustave Eiffel, whose company specialized in building metal frameworks and structures.\"\n",
        "doc = nlp(sentence)\n",
        "\n",
        "# Extract named entities\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} ({ent.label_})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie4RPkyJyM8w",
        "outputId": "e4454a7d-f3c1-41da-ea2b-5f6a2ce04cba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Eiffel Tower (LOC)\n",
            "1887 to 1889 (DATE)\n",
            "Gustave Eiffel (PERSON)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔡 Text to Vectors in NLP\n",
        "\n",
        "### 🔸 What is Text to Vectors?\n",
        "\n",
        "Text to Vectors (also called **Text Vectorization**) is the process of converting **textual data** into **numerical format** (vectors), so that machine learning models can understand and process the data.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Why Convert Text to Vectors?\n",
        "\n",
        "- Machine learning algorithms **can't work** with raw text.\n",
        "- Text must be **numerically represented** to perform classification, clustering, sentiment analysis, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 Common Text Vectorization Techniques\n",
        "\n",
        "#### 1. **Bag of Words (BoW)**\n",
        "- Creates a matrix based on **word frequency**.\n",
        "- Ignores grammar and word order.\n",
        "- Simple but can be very sparse.\n",
        "\n",
        "#### 2. **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
        "- Measures how **important** a word is in a document relative to the entire corpus.\n",
        "- Reduces weight of common words and boosts rare but meaningful ones.\n",
        "\n",
        "#### 3. **Word Embeddings**\n",
        "- Represents words as **dense vectors** in a continuous space.\n",
        "- Captures **semantic meaning** and word similarity.\n",
        "- Examples:\n",
        "  - **Word2Vec**\n",
        "  - **GloVe**\n",
        "  - **FastText**\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "81CdN-dL1Kpg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧺 Bag of Words (BoW) in NLP\n",
        "\n",
        "### 🔸 What is Bag of Words?\n",
        "\n",
        "**Bag of Words (BoW)** is a basic and popular technique to convert **text data into numerical vectors**.  \n",
        "It treats each document as a **collection (bag)** of its words, **ignoring grammar and word order**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 How It Works\n",
        "\n",
        "1. Create a **vocabulary** of all unique words in the dataset.\n",
        "2. For each document, count how many times each word from the vocabulary appears.\n",
        "3. Represent the document as a **vector of word counts**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 Example\n",
        "\n",
        "Suppose we have two sentences:\n",
        "\n",
        "- Sentence 1: `\"I love NLP\"`\n",
        "- Sentence 2: `\"NLP is fun\"`\n",
        "\n",
        "Vocabulary: `[I, love, NLP, is, fun]`\n",
        "\n",
        "Vectors:\n",
        "\n",
        "- Sentence 1 → `[1, 1, 1, 0, 0]`\n",
        "- Sentence 2 → `[0, 0, 1, 1, 1]`\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Pros\n",
        "\n",
        "- Easy to understand and implement\n",
        "- Works well with small and clean datasets\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Cons\n",
        "\n",
        "- Ignores word order and context\n",
        "- Produces **sparse vectors** (lots of zeros)\n",
        "- Doesn't capture meaning or relationships between words\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "gEMxeMNB4ITo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "EAE_43M_zgs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\n",
        "    \"he is good boy\",\n",
        "    \"she is good girl\",\n",
        "    \"he and she is good person\"\n",
        "]"
      ],
      "metadata": {
        "id": "eH7Pwl9s3cpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Initialize vectorizer with English stopwords\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(sentences)\n",
        "\n",
        "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"Bag of Words Matrix:\\n\", X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-cu86383d_K",
        "outputId": "2b7f3224-d4f3-4ae4-98f5-eeec5ed657bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['boy' 'girl' 'good' 'person']\n",
            "Bag of Words Matrix:\n",
            " [[1 0 1 0]\n",
            " [0 1 1 0]\n",
            " [0 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📚 N-grams in NLP\n",
        "\n",
        "### 🔸 What is an N-gram?\n",
        "\n",
        "An **N-gram** is a continuous sequence of **N items** (typically words or characters) from a given text.\n",
        "\n",
        "- Helps preserve some **word order** information (unlike BoW).\n",
        "- Useful in **language modeling**, **text generation**, and **feature extraction**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Types of N-grams\n",
        "\n",
        "| N-gram Type | Description                 | Example (Text: \"I love NLP\") |\n",
        "|-------------|-----------------------------|-------------------------------|\n",
        "| Unigram     | Single word (N = 1)         | `[\"I\", \"love\", \"NLP\"]`        |\n",
        "| Bigram      | Pair of words (N = 2)       | `[\"I love\", \"love NLP\"]`      |\n",
        "| Trigram     | Sequence of 3 words (N = 3) | `[\"I love NLP\"]`              |\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 Why Use N-grams?\n",
        "\n",
        "- Capture **local context** and **word order**.\n",
        "- Helps in **predictive text**, **spelling correction**, and **sentiment analysis**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Pros and Cons\n",
        "\n",
        "**✅ Pros**\n",
        "- Maintains some sequence information\n",
        "- Improves context understanding over BoW\n",
        "\n",
        "**❌ Cons**\n",
        "- Higher N → Larger feature space → More memory\n",
        "- Rare N-grams may not generalize well\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "OehB4nsA5Ckr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import library\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "bEOuLTBR3fnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define the input sentences\n",
        "sentences = [\n",
        "    \"food is good\",\n",
        "    \"food is not good\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "D0HzPq_-5AFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic Bag of Words (Unigrams = single words)\n",
        "# BoW captures only individual words and their frequency. It doesn't understand context.\n",
        "# Example: \"not good\" and \"good\" are treated as separate, unrelated words.\n",
        "bow_vectorizer = CountVectorizer()\n",
        "X_bow = bow_vectorizer.fit_transform(sentences)\n",
        "\n",
        "print(\"Unigram (BoW) Vocabulary:\")\n",
        "print(bow_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nBoW Matrix:\")\n",
        "print(X_bow.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jux5BqOa5vSb",
        "outputId": "9727a8cd-6e8b-414d-a203-65e765cc9868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram (BoW) Vocabulary:\n",
            "['food' 'good' 'is' 'not']\n",
            "\n",
            "BoW Matrix:\n",
            "[[1 1 1 0]\n",
            " [1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Bigram Vectorizer (2-word combinations)\n",
        "# Bigrams capture 2-word phrases, helping to preserve context like \"not good\" or \"food is\".\n",
        "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "X_bigram = bigram_vectorizer.fit_transform(sentences)\n",
        "\n",
        "print(\"\\nBigram Vocabulary:\")\n",
        "print(bigram_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nBigram Matrix:\")\n",
        "print(X_bigram.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqEcbEKJ5y3D",
        "outputId": "3f9f44ca-c9c7-42c0-8489-a382904bd0b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bigram Vocabulary:\n",
            "['food is' 'is good' 'is not' 'not good']\n",
            "\n",
            "Bigram Matrix:\n",
            "[[1 1 0 0]\n",
            " [1 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Trigram Vectorizer (3-word combinations)\n",
        "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
        "X_trigram = trigram_vectorizer.fit_transform(sentences)\n",
        "\n",
        "print(\"\\nTrigram Vocabulary:\")\n",
        "print(trigram_vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nTrigram Matrix:\")\n",
        "print(X_trigram.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvkJNRZG56vN",
        "outputId": "6e0f2700-7e73-4a19-f771-2986b307a438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Trigram Vocabulary:\n",
            "['food is good' 'food is not' 'is not good']\n",
            "\n",
            "Trigram Matrix:\n",
            "[[1 0 0]\n",
            " [0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📊 TF-IDF (Term Frequency-Inverse Document Frequency) in NLP\n",
        "\n",
        "### 🔸 What is TF-IDF?\n",
        "\n",
        "**TF-IDF** is a statistical measure used to evaluate the **importance of a word** in a document relative to a collection of documents (corpus).\n",
        "\n",
        "- **Term Frequency (TF)**: How frequently a word appears in a document.\n",
        "- **Inverse Document Frequency (IDF)**: How important or rare a word is across all documents in the corpus.\n",
        "\n",
        "The formula for TF-IDF is:\n",
        "\n",
        "### TF-IDF = TF * IDF\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Why Use TF-IDF?\n",
        "\n",
        "- **Highlights important words** in a document.\n",
        "- Reduces the impact of common words (e.g., \"the\", \"and\").\n",
        "- Helps capture the **relevance** of words in the context of a document set.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 How It Works\n",
        "\n",
        "1. **Term Frequency (TF)**: Measures the frequency of a word in a single document.\n",
        "   - Formula:  \n",
        "### TF = (Number of times word appears in a document) / (Total number of words in the document)\n",
        "\n",
        "\n",
        "2. **Inverse Document Frequency (IDF)**: Measures the importance of a word in the entire corpus.\n",
        "- Formula:  \n",
        "### IDF = log[(Total number of documents) / (Number of documents containing the word)]\n",
        "\n",
        "\n",
        "3. **TF-IDF**: The product of TF and IDF, giving us a score that reflects both the frequency of the word in a specific document and its rarity in the whole corpus.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Example\n",
        "\n",
        "Suppose we have the following two documents:\n",
        "\n",
        "- Document 1: `\"I love NLP\"`\n",
        "- Document 2: `\"NLP is fun\"`\n",
        "\n",
        "To calculate the TF-IDF for a word like **\"NLP\"**, we calculate the TF and IDF separately:\n",
        "\n",
        "- **TF** in Document 1 for \"NLP\":  \n",
        "`TF = 1 / 3 = 0.33`  \n",
        "(since \"NLP\" appears once in a document of 3 words)\n",
        "\n",
        "- **IDF** for \"NLP\":  \n",
        "`IDF = log(2 / 2) = 0`  \n",
        "(since \"NLP\" appears in both documents, its IDF is low)\n",
        "\n",
        "- **TF-IDF** for \"NLP\" in Document 1:  \n",
        "`TF-IDF = 0.33 * 0 = 0`  \n",
        "(indicating \"NLP\" isn't very important here due to its high frequency across documents)\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vidKEAAW61G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import the necessary library\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n"
      ],
      "metadata": {
        "id": "5qSrIlD46GR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define the sentences\n",
        "sentences = [\n",
        "    \"good boy\",\n",
        "    \"good girl\",\n",
        "    \"good boy and girl\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "LrEiZY4M8Rp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Initialize the TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Step 4: Fit and transform the sentences into TF-IDF matrix\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Step 5: Get the vocabulary (words)\n",
        "print(\"TF-IDF Vocabulary:\")\n",
        "print(tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Step 6: Print the TF-IDF matrix\n",
        "print(\"\\nTF-IDF Matrix:\")\n",
        "print(X_tfidf.toarray())\n"
      ],
      "metadata": {
        "id": "mXM0cp3_8Swg",
        "outputId": "7bff2f3b-c00f-4808-b496-80d94b8af6c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Vocabulary:\n",
            "['and' 'boy' 'girl' 'good']\n",
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.78980693 0.         0.61335554]\n",
            " [0.         0.         0.78980693 0.61335554]\n",
            " [0.63174505 0.4804584  0.4804584  0.37311881]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔠 Word2Vec in NLP\n",
        "\n",
        "### 🔸 What is Word2Vec?\n",
        "\n",
        "**Word2Vec** is a technique to represent words as **vectors** in a continuous vector space. It captures **semantic meaning** — similar words end up close together in that space.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Why Use Word2Vec?\n",
        "\n",
        "- Converts text data into a format (vectors) that ML algorithms can work with.\n",
        "- Captures **contextual and semantic** relationships between words.\n",
        "- Helps improve model performance in text classification, sentiment analysis, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 How Does Word2Vec Work?\n",
        "\n",
        "There are two training models:\n",
        "\n",
        "1. **CBOW (Continuous Bag of Words)**  \n",
        "   - Predicts a word based on context.\n",
        "   - Example: From \"The cat _ on the mat\", predict the missing word \"sits\".\n",
        "\n",
        "2. **Skip-Gram**  \n",
        "   - Predicts context words from a target word.\n",
        "   - Example: Given \"sits\", predict [\"The\", \"cat\", \"on\", \"the\", \"mat\"].\n",
        "\n",
        "---\n",
        "\n",
        "### 🔹 Advantages of Word2Vec\n",
        "\n",
        "- ✅ Learns **semantic relationships** between words (e.g., king - man + woman ≈ queen).\n",
        "- ✅ Efficient and **scalable** for large datasets.\n",
        "- ✅ Supports vector arithmetic and analogy reasoning.\n",
        "- ✅ Captures **context** in a more meaningful way than one-hot or BoW.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔸 Disadvantages of Word2Vec\n",
        "\n",
        "- ❌ Can't handle **out-of-vocabulary (OOV)** words (words not seen during training).\n",
        "- ❌ Word vectors are **static** — the meaning of a word doesn't change with context.\n",
        "  - Example: \"bank\" in \"river bank\" and \"money bank\" has the same vector.\n",
        "- ❌ Requires large corpus to train meaningful vectors.\n",
        "- ❌ Doesn't capture **morphological variations** well (e.g., \"run\", \"running\").\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "dhlJVYqGnl-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nLxn7PzpwwA",
        "outputId": "d19e4519-11e7-43ae-da08-7f0217e66cb7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Import Word2Vec\n",
        "import gensim\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "d6ejxG678UkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Create a bigger dataset (tokenized sentences)\n",
        "sentences = [\n",
        "    [\"i\", \"love\", \"natural\", \"language\", \"processing\"],\n",
        "    [\"nlp\", \"includes\", \"text\", \"preprocessing\", \"and\", \"vectorization\"],\n",
        "    [\"machine\", \"learning\", \"is\", \"fun\"],\n",
        "    [\"deep\", \"learning\", \"is\", \"part\", \"of\", \"ai\"],\n",
        "    [\"word2vec\", \"creates\", \"word\", \"embeddings\"],\n",
        "    [\"you\", \"can\", \"use\", \"cbow\", \"or\", \"skipgram\"],\n",
        "    [\"unsupervised\", \"learning\", \"is\", \"useful\", \"for\", \"text\", \"data\"],\n",
        "    [\"love\", \"this\", \"awesome\", \"nlp\", \"course\"],\n",
        "    [\"vector\", \"representations\", \"help\", \"understand\", \"meaning\"],\n",
        "    [\"language\", \"models\", \"predict\", \"next\", \"word\"]\n",
        "]"
      ],
      "metadata": {
        "id": "YXavHYSlpdbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Train Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=50, window=3, min_count=1, sg=1)\n",
        "\n",
        "# vector_size: Higher values = more info, but slower training.\n",
        "\n",
        "# window: Smaller = more local context; larger = global context.\n",
        "\n",
        "# min_count: Helps remove rare, noisy words in larger datasets.\n",
        "\n",
        "# sg: Skip-gram is better at capturing rare word meanings; CBOW is faster."
      ],
      "metadata": {
        "id": "SD8X9JwtpeKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector of a Word\n",
        "print(\"Vector for 'nlp':\")\n",
        "print(model.wv[\"nlp\"])"
      ],
      "metadata": {
        "id": "1WkJNXyTqrhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarity Between Two Words\n",
        "similarity = model.wv.similarity(\"nlp\", \"language\")\n",
        "print(\"\\nSimilarity between 'nlp' and 'language':\", similarity)"
      ],
      "metadata": {
        "id": "kwV4qOGFqsMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Most Similar Words\n",
        "print(\"\\nMost similar to 'learning':\")\n",
        "print(model.wv.most_similar(\"learning\"))"
      ],
      "metadata": {
        "id": "A5JV86fiq-bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the Odd Word Out\n",
        "odd_word = model.wv.doesnt_match([\"nlp\", \"machine\", \"apple\", \"language\"])\n",
        "print(\"\\nOdd one out:\", odd_word)"
      ],
      "metadata": {
        "id": "IWVS1f-UrDLU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}